__author__ = 'oladapobakare'

import itertools
import logging
import pprint
import sys

from gensim.models import LsiModel, TfidfModel
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # Setup logs

# Create variable for tokenizer for clarity for cachedStopWords to speed up
tokenizer = RegexpTokenizer(r'\w+')  # use the Regexp to ignore non-alpha numeric characters
cachedStopWords = stopwords.words("english")  # Use stopwords from nltk

# Read training set
with open("PM_Lg_testfile.csv", 'rb') as training_data:
    documents = training_data.read().splitlines()


# Function to convert to lowercase, convert from raw file output, split & ignore non-alpha characters (via tokenize)
def cleanconvert(input_file):
    bagofwords = []
    for doc in input_file:
        doc = doc.lower()
        doc = tokenizer.tokenize(doc)
        bagofwords_row = []
        for strings in doc:
            if strings not in cachedStopWords:
                strings = strings.decode('latin-1').encode("utf-8")  # convert from salestools.io format to utf-8
                if not str(strings).isalpha():
                    strings = ''
                bagofwords_row.append(strings)
        bagofwords.append(bagofwords_row)
    return bagofwords


# Format texts so it can be ingested by doc2bow
texts = cleanconvert(documents)
texts = list(itertools.chain(*texts))
texts = filter(None, texts)

# Store TextCorpus output
with open("Prod_Mgr_training_file.txt", 'wb') as testfile:
    testfile.write("\n".join(map(lambda x: str(x), texts)))

# Generate a training/background corpus from LinkedIn Scrapes
from gensim.corpora import TextCorpus, MmCorpus, Dictionary

background_corpus = TextCorpus(input="Prod_Mgr_training_file.txt")
background_corpus.dictionary.save("lsi_test_dict.dict")  # save dictionary generated by the corpus
MmCorpus.serialize("background_corpus.mm", background_corpus)  # use numpy to persist corpus

# Using persisted corpus
bow_corpus = MmCorpus("background_corpus.mm")
dictionary = Dictionary.load("lsi_test_dict.dict")  # load a dictionary

tfidf_transformation = TfidfModel(background_corpus, id2word=dictionary)
tfidf_transformation.save("tfidf_500.model")

# Read file to be queried
with open("input_resume_3.txt", "rb") as inputFile:
    sampleResume = inputFile.read().splitlines()
sampleResume = cleanconvert(sampleResume)

# Format texts and file to be queried
blanks = []
blanks.append(texts)
texts = blanks
texts = filter(None, texts)
sampleResume = list(itertools.chain(*sampleResume))
sampleResume = filter(None, sampleResume)

# Transform input file to be queried to vector space
bow_sampleResume = [dictionary.doc2bow(sampleResume) for document in sampleResume]
tfidf_sampleResume = tfidf_transformation[bow_sampleResume]

lsi_transformation = LsiModel(corpus=tfidf_transformation[bow_corpus], id2word=dictionary, num_topics=500)
lsi_transformation.save("lsi_500.model")
lsi_transformation = LsiModel.load("lsi_500.model")

from gensim.similarities import Similarity

index_documents = texts

corpus = [dictionary.doc2bow(text) for text in texts]

index = Similarity(corpus=lsi_transformation[tfidf_transformation[corpus]], num_features=500, output_prefix="shard")

# Save then load(to test) similarity index
index.save("similarity_index_500")
index = Similarity.load("similarity_index_500")

sims_to_query = index[lsi_transformation[tfidf_sampleResume]]

best_score = max(sims_to_query)

pprint.pprint(best_score)

print best_score[0]
